{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9894746,"sourceType":"datasetVersion","datasetId":6072595}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Diffusion. Text-to-Image Generation.\n\n- Liana Mardanova\n- DS-01\n- l.mardanova@innopolis.university","metadata":{}},{"cell_type":"markdown","source":"## Generating Beautiful Interiors with Diffusion Models\n\nIn this project, I undertook the task of training a diffusion model to generate visually appealing interior designs. The notebook is systematically organized into four key sections:\n\n1. **Preparations**\n2. **Dataset**\n3. **Model**\n4. **Results**\n5. **Resources**\n\n### Challenges \n\n1) Understanding Diffusion Models\n   \nGrasping the complex mechanisms and underlying principles of diffusion models was initially challenging. To overcome this, I relied on comprehensive explanations from resources such as [this explanatory video](https://youtu.be/HoKDTa5jHvg?si=xoofJvnZOjTeeR5M) and a detailed tutorial on training conditional diffusion models from scratch provided by [Weights & Biases](https://wandb.ai/capecape/train_sd/reports/How-To-Train-a-Conditional-Diffusion-Model-From-Scratch--VmlldzoyNzIzNTQ1).\n\n2) Sourcing an Appropriate Dataset\n   \nFiltering interior images from the COCO dataset using the keyword \"interior\" resulted in poor-quality data, necessitating the creation of a custom dataset. Consequently, I opted to create a [custom dataset](https://www.kaggle.com/datasets/liaaana/ikea-interiors/data) using resources from [GitHub](https://github.com/IvonaTau/ikea/tree/master/images/room_scenes), ensuring a higher quality and more relevant collection of interior images.\n\n3) Long Training\n   \nTraining a diffusion model from scratch was time-consuming, which led to the adoption of LoRA (Low-Rank Adaptation) to achieve high-quality results more efficiently. This approach significantly reduced the training time without compromising the model's performance. This [tutorial](https://www.kaggle.com/code/ostamand/stable-diffusion-1-5-lora-fine-tuning) was very helpful.\n\nIn summary, this project demonstrates that we can successfully leverage the capability of diffusion models in generating beautiful interior designs.","metadata":{}},{"cell_type":"markdown","source":"## 0. Preparations","metadata":{}},{"cell_type":"code","source":"# install nessassary packages\n!pip install diffusers peft transformers -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:10.329823Z","iopub.execute_input":"2024-11-15T16:23:10.330793Z","iopub.status.idle":"2024-11-15T16:23:25.562180Z","shell.execute_reply.started":"2024-11-15T16:23:10.330733Z","shell.execute_reply":"2024-11-15T16:23:25.561106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# imports\nfrom torch.utils.data import random_split, DataLoader\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import Dataset\nfrom pathlib import Path\nfrom diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, DiffusionPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom huggingface_hub import login\nfrom peft import LoraConfig\nimport torch\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport math\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom peft.utils import get_peft_model_state_dict\nfrom diffusers.utils import convert_state_dict_to_diffusers\nfrom datasets import load_dataset\nfrom functools import partial\nfrom PIL import Image\nfrom kaggle_secrets import UserSecretsClient\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom pydantic import BaseModel\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport pandas as pd\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:25.564439Z","iopub.execute_input":"2024-11-15T16:23:25.564754Z","iopub.status.idle":"2024-11-15T16:23:46.356425Z","shell.execute_reply.started":"2024-11-15T16:23:25.564721Z","shell.execute_reply":"2024-11-15T16:23:46.355639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TrainingConfig(BaseModel):\n    data_dir: str = '/kaggle/input/ikea-interiors'\n    csv_file: str = 'information.csv'\n    image_path_column: str = 'path'\n    description_cloumn: str = 'description'\n    image_size: int = 512\n    lr: float = 0.0001\n    batch_size: int = 4\n    rank: int = 120\n    max_grad_norm: float = 1.0\n    pretrained_model_name: str = \"runwayml/stable-diffusion-v1-5\"\n    data_dir: str = \"/kaggle/input/ikea-interiors\"\n    seed: int = 42\n    inference_steps: int = 50\n    num_epochs: int = 30","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:46.357512Z","iopub.execute_input":"2024-11-15T16:23:46.358083Z","iopub.status.idle":"2024-11-15T16:23:46.446606Z","shell.execute_reply.started":"2024-11-15T16:23:46.358049Z","shell.execute_reply":"2024-11-15T16:23:46.445877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = TrainingConfig()\ntorch.manual_seed(config.seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:46.447579Z","iopub.execute_input":"2024-11-15T16:23:46.447845Z","iopub.status.idle":"2024-11-15T16:23:46.492188Z","shell.execute_reply.started":"2024-11-15T16:23:46.447816Z","shell.execute_reply":"2024-11-15T16:23:46.491177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Dataset","metadata":{}},{"cell_type":"code","source":"def display_images_with_descriptions(data_dir, csv_file, image_path_column, description_column, num_images=3):\n    \"\"\"\n    Display a specified number of images from a folder with descriptions in two columns.\n    \"\"\"\n    df = pd.read_csv(Path(data_dir) / csv_file)\n    fig, axs = plt.subplots(num_images, 2, figsize=(10, 4 * num_images))\n    \n    for i in range(num_images):\n        image_path = Path(data_dir) / df.iloc[i][image_path_column]\n        description = df.iloc[i][description_column]\n        \n        image = Image.open(image_path).convert(\"RGB\")\n        axs[i, 0].imshow(image)\n        axs[i, 0].axis(\"off\")\n        \n        axs[i, 1].text(0.5, 0.5, description, ha=\"center\", va=\"center\", wrap=True, fontsize=12)\n        axs[i, 1].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:46.494733Z","iopub.execute_input":"2024-11-15T16:23:46.495076Z","iopub.status.idle":"2024-11-15T16:23:46.502553Z","shell.execute_reply.started":"2024-11-15T16:23:46.495043Z","shell.execute_reply":"2024-11-15T16:23:46.501644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_images_with_descriptions(config.data_dir, config.csv_file, config.image_path_column, config.description_cloumn, num_images=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:46.503807Z","iopub.execute_input":"2024-11-15T16:23:46.504183Z","iopub.status.idle":"2024-11-15T16:23:47.630142Z","shell.execute_reply.started":"2024-11-15T16:23:46.504141Z","shell.execute_reply":"2024-11-15T16:23:47.629092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class IkeaDataset(Dataset): \n    def __init__(self, data_dir, csv_file, tokenizer, image_path_column, description_column, image_size):\n        self.data_dir = Path(data_dir)\n        self.df = pd.read_csv(self.data_dir / csv_file)\n        self.tokenizer = tokenizer\n        self.image_path_column = image_path_column\n        self.description_column = description_column\n        self.image_size = image_size\n\n        self.transform = transforms.Compose([\n            transforms.RandomCrop((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]),\n        ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_path = self.data_dir / self.df.iloc[idx][self.image_path_column]\n        description = self.df.iloc[idx][self.description_column]\n        image = self.transform(Image.open(image_path).convert(\"RGB\"))\n        input_ids = self.tokenizer(\n            description,\n            max_length=self.tokenizer.model_max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )[\"input_ids\"][0]\n\n        return {\"pixel_values\": image, \"input_ids\": input_ids, \"description\": description}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:47.631351Z","iopub.execute_input":"2024-11-15T16:23:47.631670Z","iopub.status.idle":"2024-11-15T16:23:47.641102Z","shell.execute_reply.started":"2024-11-15T16:23:47.631637Z","shell.execute_reply":"2024-11-15T16:23:47.640147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Model","metadata":{}},{"cell_type":"code","source":"# helper functions\ndef load_model(model_name):\n    tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n    text_encoder = CLIPTextModel.from_pretrained(\n        model_name, subfolder=\"text_encoder\", torch_dtype=torch.float16\n    )\n    vae = AutoencoderKL.from_pretrained(\n        model_name, subfolder=\"vae\", torch_dtype=torch.float16\n    )\n    scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n    unet = UNet2DConditionModel.from_pretrained(\n        model_name, subfolder=\"unet\", torch_dtype=torch.float16\n    )\n    return tokenizer, text_encoder, vae, scheduler, unet\n\ndef freeze_parameters(model):\n    for param in model.parameters():\n        param.requires_grad = False\n\ndef change_parameters_type(model):\n    for param in model.parameters():\n        if param.requires_grad:\n            param.data = param.to(dtype=torch.float32)\n\ndef get_lora_parameters(model):\n    return [param for param in filter(lambda param: param.requires_grad, [param for param in model.parameters()])]\n\n\ndef get_models(model_name, dtype=torch.float16):\n    tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n    text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\").to(dtype=dtype)\n    vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\").to(dtype=dtype)\n    scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n    unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\").to(dtype=dtype)\n    return tokenizer, text_encoder, vae, scheduler, unet\n\ndef setup_models_for_training(model_name, rank: int=128):\n    tokenizer, text_encoder, vae, scheduler, unet = load_model(model_name)\n    \n    freeze_parameters(text_encoder)\n    freeze_parameters(vae)\n    freeze_parameters(unet)\n    \n    unet_lora_config = LoraConfig(\n        r=rank,\n        lora_alpha=rank,\n        init_lora_weights=\"gaussian\",\n        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n    )\n\n    unet.add_adapter(unet_lora_config)\n\n    change_parameters_type(unet)\n\n    return tokenizer, text_encoder, vae, scheduler, unet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:47.642479Z","iopub.execute_input":"2024-11-15T16:23:47.643185Z","iopub.status.idle":"2024-11-15T16:23:47.656810Z","shell.execute_reply.started":"2024-11-15T16:23:47.643128Z","shell.execute_reply":"2024-11-15T16:23:47.655946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# simplified funciton from tutorial\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\ndef train(\n    tokenizer, \n    text_encoder, \n    vae, \n    scheduler, \n    unet,\n    train_dataset, \n    train_dataloader,\n    config,\n    device\n):        \n    lora_params = get_lora_parameters(unet)\n\n    text_encoder.to(device).eval()\n    vae.to(device).eval()\n    unet.to(device).train()\n\n    optimizer = torch.optim.AdamW(lora_params, lr=config.lr)\n    scaler = torch.cuda.amp.GradScaler()\n    losses = []\n\n    for epoch in range(config.num_epochs):\n        epoch_loss = 0.0\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{config.num_epochs}\", unit=\"batch\")\n        \n        for batch in progress_bar:\n            bs = batch[\"input_ids\"].size(0)\n\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                with torch.no_grad():\n                    encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(device), return_dict=False)[0]\n    \n                timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bs,)).long().to(device)\n    \n                with torch.no_grad():\n                    latents = vae.encode(batch[\"pixel_values\"].to(device)).latent_dist.sample()\n                    latents = latents * vae.config.scaling_factor\n\n                noise = torch.randn_like(latents)\n                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n    \n                loss = F.mse_loss(noise_pred, noise, reduction=\"mean\")\n\n            scaler.scale(loss).backward()\n\n            if config.max_grad_norm > 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(lora_params, config.max_grad_norm)\n            \n            scaler.step(optimizer)\n            scaler.update()\n\n            loss_value = loss.item()\n            epoch_loss += loss_value\n            \n            progress_bar.set_postfix({\"Loss\": f\"{loss_value:.4f}\"})\n\n        avg_epoch_loss = epoch_loss / len(train_dataloader)\n        losses.append(epoch_loss)\n        print(f\"Epoch {epoch + 1}/{config.num_epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n\n    return losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:47.658101Z","iopub.execute_input":"2024-11-15T16:23:47.658412Z","iopub.status.idle":"2024-11-15T16:23:47.674064Z","shell.execute_reply.started":"2024-11-15T16:23:47.658374Z","shell.execute_reply":"2024-11-15T16:23:47.673286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    del models, pipe\n    import gc; gc.collect()\n    torch.cuda.empty_cache()\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:47.675220Z","iopub.execute_input":"2024-11-15T16:23:47.675604Z","iopub.status.idle":"2024-11-15T16:23:47.684732Z","shell.execute_reply.started":"2024-11-15T16:23:47.675550Z","shell.execute_reply":"2024-11-15T16:23:47.683884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = setup_models_for_training(config.pretrained_model_name, rank=config.rank)\ntrain_dataset = IkeaDataset(\n    Path(config.data_dir), \n    config.csv_file, \n    models[0], \n    config.image_path_column, \n    config.description_cloumn, \n    config.image_size\n)\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=config.batch_size, \n    shuffle=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:23:47.685845Z","iopub.execute_input":"2024-11-15T16:23:47.686188Z","iopub.status.idle":"2024-11-15T16:24:15.208778Z","shell.execute_reply.started":"2024-11-15T16:23:47.686156Z","shell.execute_reply":"2024-11-15T16:24:15.207804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"losses = train(\n    *models,\n    train_dataset, \n    train_dataloader,\n    config,\n    device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T16:24:15.210064Z","iopub.execute_input":"2024-11-15T16:24:15.210412Z","iopub.status.idle":"2024-11-15T17:30:39.465940Z","shell.execute_reply.started":"2024-11-15T16:24:15.210376Z","shell.execute_reply":"2024-11-15T17:30:39.464920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(losses)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:30:39.468331Z","iopub.execute_input":"2024-11-15T17:30:39.468648Z","iopub.status.idle":"2024-11-15T17:30:39.660337Z","shell.execute_reply.started":"2024-11-15T17:30:39.468615Z","shell.execute_reply":"2024-11-15T17:30:39.659386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(models[-1]))\nStableDiffusionPipeline.save_lora_weights(\n    save_directory=\"/kaggle/working/\",\n    unet_lora_layers=unet_lora_state_dict,\n    safe_serialization=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:30:39.663486Z","iopub.execute_input":"2024-11-15T17:30:39.663799Z","iopub.status.idle":"2024-11-15T17:30:39.826357Z","shell.execute_reply.started":"2024-11-15T17:30:39.663766Z","shell.execute_reply":"2024-11-15T17:30:39.825535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Results","metadata":{}},{"cell_type":"code","source":"# helper functions\ndef generate(pipeline, prompt, seed, num_inference_steps):\n    generator = torch.Generator(device=device).manual_seed(seed)\n    result = pipeline(prompt, num_inference_steps=num_inference_steps, generator=generator).images\n    return result[0] \n\ndef display_multiple_results(image_pairs, prompts):\n    \"\"\"\n    Display pairs of generated images side by side with a title for each image\n    and a prompt displayed above each pair.\n    \"\"\"\n    titles = [\"Pretrained Model\", \"New Model\"]\n    for images, prompt in zip(image_pairs, prompts):\n        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n        fig.suptitle(prompt, fontsize=16, y=1.05)\n\n        for ax, img, title in zip(axs, images, titles):\n            ax.imshow(img)\n            ax.set_title(title)\n            ax.axis('off')\n        \n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:30:39.827596Z","iopub.execute_input":"2024-11-15T17:30:39.827940Z","iopub.status.idle":"2024-11-15T17:30:39.835767Z","shell.execute_reply.started":"2024-11-15T17:30:39.827890Z","shell.execute_reply":"2024-11-15T17:30:39.834754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = DiffusionPipeline.from_pretrained(\n    config.pretrained_model_name,\n    torch_dtype=torch.float16\n).to(device)\n\npipe_new = DiffusionPipeline.from_pretrained(\n    config.pretrained_model_name,\n    torch_dtype=torch.float16\n).to(device)\npipe_new.load_lora_weights(\"/kaggle/working/pytorch_lora_weights.safetensors\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:30:39.836836Z","iopub.execute_input":"2024-11-15T17:30:39.837184Z","iopub.status.idle":"2024-11-15T17:30:54.537805Z","shell.execute_reply.started":"2024-11-15T17:30:39.837140Z","shell.execute_reply":"2024-11-15T17:30:54.536887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = [\n\n    # Bedroom\n    \"Industrial IKEA bedroom with exposed brick walls, a light gray bed frame, and a dark metal headboard. Black metal pendant lighting adds a bold touch to the room’s industrial charm\",\n    \"Boho IKEA bedroom with soft white walls, light wood furniture, and dark woven accents. A dark wood bed frame and metal lamps create an inviting, earthy ambiance\",\n    \"IKEA bedroom with a light teal accent wall, soft white bedding, and dark wood bedside tables. Black metal wall sconces add modern contrast to the calming color scheme\",\n    \"Contemporary IKEA bedroom with light walls and a dark gray upholstered bed, highlighted by black metal side tables and lamps. Soft lighting brings warmth to the modern design\",\n    \"Minimalist IKEA bedroom with light walls, white bedding, and a dark wooden headboard. Black metal frames around mirrors and picture frames add structure to the minimalist space\",\n    \n    # Kitchen\n    \"IKEA kitchen with pale blue cabinets, light countertops, and a dark wood kitchen island. Black metal handles and a metal range hood add modern industrial accents\",\n    \"Rustic IKEA kitchen with white walls and light wood shelving, contrasted by dark cabinetry and black metal stools. Exposed metal accents add a farmhouse feel\",\n    \"Modern IKEA kitchen with light gray walls, white countertops, and dark wood cabinets. Black metal shelving and pendant lights add a sleek industrial vibe\",\n    \"IKEA kitchen with light wood cabinetry and a dark gray backsplash. Black metal hardware and open shelves bring an edgy contrast to the warm wood tones\",\n    \"Minimalist IKEA kitchen with white walls, light wood countertops, and dark green cabinets. Black metal light fixtures complete the modern, airy look\",\n    \n    # Living Room\n    \"Coastal IKEA living room with light blue walls, a beige sofa, and a dark wood coffee table. Black metal decor and lighting add contrast to the beachy vibe\",\n    \"Modern IKEA living room with light gray walls and a white sectional, contrasted by a dark wood TV stand and black metal accents. The minimal decor adds a serene feel\",\n    \"IKEA living room with light, neutral walls and a beige sofa, paired with dark wood end tables and black metal lighting. Green plants add a pop of color to the calm space\",\n    \"Contemporary IKEA living room with white walls, a light gray rug, and a dark wood coffee table. Black metal decor enhances the modern, minimalist feel\",\n    \"Rustic IKEA living room with light beige walls, a cream sofa, and dark wood shelves. Black metal frames and warm lighting add an industrial touch to the cozy design\",\n    \n    # Dining Room\n    \"Scandinavian IKEA dining room with light wood furniture, white walls, and a dark wood dining table. Black metal pendant lights create a bold contrast in the airy room\",\n    \"Minimalist IKEA dining room with pale gray walls, a light wood table, and dark wood chairs. Black metal light fixtures bring structure to the soft space\",\n    \"Rustic IKEA dining room with white walls, a light wood dining set, and dark metal chairs. The natural textures add warmth to the bright space\",\n    \"Modern IKEA dining room with dark gray walls, a light wood table, and black metal accents. Sleek, minimalist decor enhances the modern aesthetic\",\n    \"Cozy IKEA dining room with light beige walls, a white table, and dark wood chairs. Black metal pendant lighting adds warmth to the inviting space\",\n    \n    # Office\n    \"Scandinavian IKEA office with light wood furniture, white walls, and a dark wood desk. Black metal shelving adds a modern, functional element to the minimalist design\",\n    \"Industrial IKEA office with exposed brick walls, a light wood desk, and black metal accents. Dark wood shelving enhances the urban, professional feel\",\n    \"Modern IKEA office with white walls, a light wood desk, and a dark gray accent wall. Black metal decor adds contrast to the sleek, minimalist space\",\n    \"Cozy IKEA office with light walls, a white desk, and a dark wood bookshelf. Black metal frames and light fixtures bring structure to the comfortable work area\",\n    \"Minimalist IKEA office with light gray walls, a white desk, and dark wood accents. Black metal lighting fixtures complete the calm, focused workspace\",\n\n]\n\nimage_pairs = []\nfor prompt in prompts:\n    images = []\n    images.append(generate(pipe, prompt, config.seed, config.inference_steps))\n    images.append(generate(pipe_new, prompt, config.seed, config.inference_steps))\n    image_pairs.append(images)\n\ndisplay_multiple_results(image_pairs, prompts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T17:30:54.539616Z","iopub.execute_input":"2024-11-15T17:30:54.540333Z","iopub.status.idle":"2024-11-15T17:43:16.288851Z","shell.execute_reply.started":"2024-11-15T17:30:54.540289Z","shell.execute_reply":"2024-11-15T17:43:16.287761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Results\n### Key Observations\n\n1. **Resemblance to IKEA Furniture**  \n   - Generated objects and furniture exhibit visual similarities to IKEA's design style, reflecting the model's ability to learn key furniture characteristics from the dataset.\n\n2. **Perspective and Composition**  \n   - The model successfully replicates the perspective and framing typical of IKEA's interior photography, indicating its grasp of spatial and compositional features.\n\n3. **Performance by Room Type**  \n   - **Living Rooms and Kitchens**: These room types are the most refined, often generating aesthetically pleasing and cohesive designs resembling catalog-quality images.  \n   - **Bedrooms, Offices and Dining rooms**: The model's performance in generating these room types is less consistent, with occasional artifacts or unrealistic elements.\n\n### Areas for Improvement\n\n1. **Extended Training Duration**  \n   - Increasing training time could improve the model's ability to capture finer details and produce more coherent results across all room types.\n\n2. **Dataset Quality and Size**  \n   - Expanding the dataset to include a larger variety of high-quality interior images would enhance the model's ability to generalize and improve design diversity.\n\n3. **Enhanced Metadata**  \n   - Improving the quality and specificity of dataset descriptions could help the model generate designs that are more accurate and contextually appropriate.\n\n### Conclusion\n\nThe project highlights the potential of diffusion models for generating beautiful interior designs. While the results are promising, further training and dataset refinements could significantly enhance the model's capabilities, especially for challenging room types like bedrooms and offices.\n","metadata":{}},{"cell_type":"markdown","source":"## 5. Resources\n- https://www.kaggle.com/code/ostamand/stable-diffusion-1-5-lora-fine-tuning\n- https://github.com/IvonaTau/ikea/tree/master/images/room_scenes\n- https://www.kaggle.com/datasets/liaaana/ikea-interiors/data\n- https://wandb.ai/capecape/train_sd/reports/How-To-Train-a-Conditional-Diffusion-Model-From-Scratch--VmlldzoyNzIzNTQ1\n- code from Lab 8 in F24-PMLDL course","metadata":{}}]}